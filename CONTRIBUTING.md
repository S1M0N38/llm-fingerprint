# Contributing

Welcome to the LLM Fingerprinting project! This project aims to develop methods for identifying language models based on their response patterns. We appreciate your interest in contributing.

This document provides guidelines and instructions for contributing to the project. Whether you're fixing bugs, improving documentation, or proposing new features, your contributions are welcome.

## How to Contribute

1. **Report Issues**: If you find bugs or have feature requests, please create an issue on GitHub.

2. **Submit Pull Requests**: For code contributions, fork the repository, make your changes, and submit a pull request.

3. **Follow Coding Standards**: We use Ruff for linting and formatting, and pyright/basedpyright for type checking. Make sure your code passes all checks.

4. **Write Tests**: For new features or bug fixes, please include tests to validate your changes.

5. **Use Conventional Commits**: Follow the conventional commits specification for your commit messages.

## Environment Setup

This section describes how to set up the **recommended** development environment for this project [uv](https://docs.astral.sh/uv/).

1. Download the repository:

```sh
git clone https://github.com/S1M0N38/llm-fingerprint.git
cd llm-fingerprint
```

2. Create environment:

```sh
uv venv
uv sync --dev
```

3. Set up environment variables:

```sh
cp .envrc.example .envrc
# And modify the .envrc file
```

The environment setup is now ready to use. Every time you are working on the project, you can activate the environment by running:

```sh
source .envrc
```

> You can use [direnv](https://github.com/direnv/direnv) to automatically activate the environment when you enter the project directory.

## Project Structure

```
.
├── config/                  # Configuration files
│   └── llama-cpp.yaml       # Configuration for llama.cpp models
├── data/                    # Data directories
│   ├── chroma/              # ChromaDB storage
│   ├── prompts/             # Prompts for model testing
│   └── samples/             # Generated samples from models
├── src/                     # Source code
│   └── llm_fingerprint/     # Main package
│       ├── __init__.py      # Package initialization
│       ├── cli.py           # Command line interface
│       ├── generate.py      # Sample generation logic
│       ├── models.py        # Data models using Pydantic
│       ├── query.py         # Query logic for model identification
│       └── upload.py        # Upload logic for ChromaDB
├── tests/                   # Test files (not yet implemented)
├── .envrc.example           # Example environment variables
├── .gitattributes           # Git attributes configuration
├── .gitignore               # Git ignore configuration
├── .pre-commit-config.yaml  # Pre-commit hooks configuration
├── .python-version          # Python version specification
├── CHANGELOG.md             # Project changelog
├── CONTRIBUTING.md          # Contribution guidelines
├── LICENSE                  # MIT License
├── README.md                # Project documentation
├── justfile                 # Command runner configuration
├── pyproject.toml           # Python project configuration
└── uv.lock                  # Dependencies lock file
```

### Core Components

1. **CLI (`cli.py`)**: Provides command-line interface with three main commands:

   - `generate`: Creates samples from LLMs using standard prompts
   - `upload`: Uploads samples to ChromaDB and creates centroids
   - `query`: Identifies unknown models by comparing with known fingerprints

2. **Data Models (`models.py`)**: Defines Pydantic models for data structures:

   - `Prompt`: Stores prompts used for testing models
   - `Sample`: Contains completions generated by LLMs
   - `Result`: Stores model identification results

3. **Sample Generation (`generate.py`)**: Handles generating responses from LLMs

   - Uses OpenAI-compatible APIs to request completions
   - Supports various models through configuration

4. **Database Interface (`upload.py`, `query.py`)**:

   - Uploads samples to ChromaDB for vector storage
   - Creates centroids for model-prompt combinations
   - Queries vectors to identify unknown models

5. **Configuration**:
   - Environment variables for API keys and endpoints
   - Configuration files for local model servers

### Workflow

The typical workflow involves:

1. Generating samples from known models using fixed prompts
2. Uploading these samples to a vector database (ChromaDB)
3. Creating centroids for each model-prompt combination
4. Testing unknown models with the same prompts
5. Comparing vectors to identify the most similar known model
